TODO:
- Jerome: try a non-Gaussian (=pathological) error term
- Lasso/plot: show vertical of automatically selected model
- classification: balanced accuracy



1. FEV - prediction ignores the significant ones
2. birth weight -
3. Prostate - not significant but predictive
-> leave simulations + classification for the exectuable notebooks only?



Titles:
On the inference-prediction dilemma in biomedicine


Main conclusions
1) across simulations and applications, inference/GLM tends to yield more false positives, while prediction yielded more false negatives (coherent with the parsimony constraint of the Lasso)
2) all four possible cases occur in practice:
  * significant and predictive
  * significant but not predictive
  * not significant but predictive
  * not significant and not predictive
3) Efron & Tibshirani 1993: Statististcians work at the intersection of empirical research,
science, and philosophy -> this project tried to emphasize the importance of the philsophical
component of medical research activity. Just because the methods that we use automatically
generate results does not preclude the necessity of questioning the obtained results and
conclusions; just because a tool gives answsers, does not mean that the tool has been an
appropriate choice for the underlying research question





Discussion:

- vocabulary/expressions: we are in the usual linear regression setup; the selection event; 

- biggest abberations between prediction and inference when a) the assumed model did not correspond to the ground-throuth process that generated the data and b) correlation between the input variables -> however, this was likely to be the most realistic simulation in medicine, neuroscience, and genetics where we have complex biology-phenotype interactions that are unlikely to correspond to the linear GLM models that we use in everyday work

- potentially related to the discussion of whether pvales for prediction performances make sense: cf. Evernote on pvalues in DNNs

- The lasso with k nonzero coefficients has k degrees of freedom in expectation




Discussion with Olivier March 8:

- our goal is different from post-selection inference: In post-sel inf we have already gone through a filter process, whether it be least-angle regression or PCA or Lasso, after which we want to know among the chosen subset of variables which are *also* significant

- "modelling error" (vs. "measurement error"): The most common case in biomedicine is probably that the assumed linear model is wrong and does not correspond to the real data mechanisms of how the observations arose. In genetics for example, using logistic-regression-like GWAS analyses we can often only explain very small fractions of the variance related to the disease phenotype in psychiatry and other complex dieases:

We looked for both additive effects which can be seen as the combined effect produced by different symptoms on schizophrenia severity is equal to the sum of their separate effect and interaction effects which mean that the combined effect is not additive. In fact, it is widely assumed that higher-order interactions between vulnerabilities triggered by the environment such as growing up in an urbanized area (Van Os and Kapur, 2009) and vulnerabilities conferred by genes such as NRG1 (Harrison and Weinberger, 2005) are important in the etiology of schizophrenia and may result in this major psychiatric disorder (Van Os and Kapur, 2009). Nonetheless, the very successful genome-wide association studies (GWAS) have been mostly grounded in additive models and thus blind to such interaction effects. In other words, common GWAS applications investigate the separate effect of each individual gene on overall disease vulnerability.




Bertrand March 9:
- random forests -> feature importance
- applications: cross-correlations -> significance is divided between the variables -> more difficult for each individual variable to be significant
- shrinkage ~ few variables
- forward step-wise selection for selection based on p-value (instead of prediction): faisable
- SNR: L2(X_beta) / L2(epsilon) -> ratio more informative
- Lasso: does shrinkage AND selction -> difficult to separate -> once the non-zero betas are set, one could refit an OLS bsed on the subset of identified betas for evaluation out of sample because this would allow to separate/disembiguate/disentangle shrinkage and selection in the evaluation of an out-of-sample performance
- 100 samples, 40 variables, error = none -> we should still have 5% false positives
- aberration by pertubration is weak vs strong condiction on: monotony preserved or not ?
- post-selection inference does not really work and the solutions from Stanford should not be really trusted
- perturbation: I would apply that to Y rather than specific input variables [In sklearn example: perturbation is however also applied to X directly]
- your simulations are long-data scenarios that are not settings where we would naturally choose a Lasso -> not 100% naturalistic setting
- a fundamemntal issue is that significance and variable selection are *binary* metrics, while the process and the underlying computation is actually continuous in nature
-> OLS/inference has literally no researchers degrees of freedom <=> but there are naturally so many more researchers degrees of freedom when computing Lasso, CV, and choosing the lambda
-> the simulations are less convincing than the applications to the real datasets: you should understand the real data better (SNR can be computed even if the ground truth model is not known + quantify the correlation between the inputs by cross-correlation plots)
-> compare to random forests feature importances because these do not have shrinkage effects





Daniel Margulies March 11:
- significant variables are not automatically predictive? Why not? That is wierd?



Discussion with Olivier March 12:
- "forward"-variant of RFE with RF
- Partial dependence plots on variables suspected to have non-linear 
- true vs predicted / plot avec OLS and optimal Lasso and RF
- RF better than Lasso -> indicators of non-linearity
- MAD is more interpretable
- polynomial expansion -> feature selection f_regression -> test
- parameter search with gradient boosting











Discussion with Gael:
- first selected variable in fwd stepwise and Lasso is expectedly the same, because there they are selected marginally; since there is not other potetnial in the model yet -> then bias of L1 will increase in the sense of bias/variance tradeoff
- prediction: the point is this: Do I bring in new information to solve my prediction problem or not? So this is related to the ground truth correlation structure of the input features
- let's write the paper in GDocs please, this is the most practical for me













Applications:
Mini-mental / CamCAN
/ HCP
/ UKBB



Target journals:
- Nature Methods
- PLoS Medicine (IF 13.5)
- PNAS



JBP? Nichols?
