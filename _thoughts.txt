TODO:
- Jerome: try a non-Gaussian (=pathological) error term
- Lasso/plot: show vertical of automatically selected model
- classification: balanced accuracy
- do ypredicted versus ytrue 45 degree plots on particular examples
- compute+plot also out-of-sample prediction performance for the linear regression
- plot in-sample R2 OSL (goodness of fit) versus out-of-sample R2 from Lasso
- simulations: compute effect size + true R2 + bias-variance decomposition
- Consider in Lasso/accuracy plot: add horizontal line for OLS train+test R2 (like in Yarkoni/Westfall)




1. FEV - prediction ignores the significant ones
2. birth weight -
3. Prostate - not significant but predictive
-> leave simulations + classification for the exectuable notebooks only?



Titles:
On the prediction-inference dilemma in biomedicine


keywords: statistical significance | prediction | high-dimensional data | variable selection



Main conclusions
1) across simulations and applications, inference/GLM tends to yield more false positives, while prediction yielded more false negatives (coherent with the parsimony constraint of the Lasso)
2) all four possible cases occur in practice:
  * significant and predictive
  * significant but not predictive
  * not significant but predictive
  * not significant and not predictive
3) Efron & Tibshirani 1993: Statististcians work at the intersection of empirical research,
science, and philosophy -> this project tried to emphasize the importance of the philsophical
component of medical research activity. Just because the methods that we use automatically
generate results does not preclude the necessity of questioning the obtained results and
conclusions; just because a tool gives answsers, does not mean that the tool has been an
appropriate choice for the underlying research question

- the present example is a simplified version of many current real-world settings: in the high-dimensional settings, the divergence between prediction and inference can be expected to be even bigger

-> "Modeling for prediction and modeling for inference ask research questions in importantly different ways."
-> divergende is already relevant in analysis regimes with few variables
-> 21st century data analysts in biomedicine should embrance both inferential and preditive modeling for their respective merits: Donoho2015




Interesting points from the literature:
- "The tension between machine learning and academic statistics "
- "predictive variable set" <-> "significant variable set"
- "real and well-known medical datasets"
- "we offer some statistical insight"
-"expose some differences between"
-"P values and accompanyingmethods"
- "illuminate examples"
- "adoption of other inference tools if appropriate"
- Efron1991: "Most scientists face problems of data analysis. What can I conclude form my data? How far can I trust the conclusions?"
- Breiman: " ‚ÄúImportance‚Äù does not yet have a satisfactory the- oretical definition (I haven‚Äôt been able to locate the article Brad references but I‚Äôll keep looking). It depends on the dependencies between the output variable and the input variables, and on the depen- dencies between the input variables. The problem begs for research.
"
-Ioannidis2018/JAMA: "With the advent of big data, statistical significance will increasingly mean very little because extremely low P values are routinely obtained for signals that are too small to be useful even if true."
- ‚ÄûWhat meaning can one give to statements that ‚Äúvariable X is important or not impor- tant.‚Äù This has puzzled me on and off for quite a while‚Ä¶ variable importance has always been defined operationally.  My definition of variable importance is based on prediction. A variable might be considered important if deleting it seriously affects prediction accuracy.
-From clinical trials to epidemiology, p-values have long been used to back claims for the discovery of real e ects amid noisy data.
-Many journalists spotted the obvious connection between the unreliability of p-values and one of the biggest scienti c controversies of our time: the replication crisis,
-as Wasserstein has put it, to ‚Äústeer research into a post p < 0.05 era‚Äù. 
-prediction is causin of ifnerence, not a brother or sister -> inference is a differently nuanced alternative
-keyword: reproducibility
--ASA:  Statistical significance is not equivalent to scientific, human, or economic significance. Smaller p-values do not necessarily imply the presence of larger or more important effects, and larger p- values do not imply a lack of importance or even lack of effect. Any effect, no matter how tiny, can produce a small p-value if the sample size or measurement precision is high enough, and large effects may produce unimpressive p-values if the sample size is small or measurements are imprecise. Similarly, identical estimated effects will have different p-values if the precision of the estimates differs.
-ASA: "No single index should substitute for scientific reasoning."
-Gelman: "really, any p-value) can be viewed as a crude measure of sample size, and this can be framed as the distinction between"
-ASA/Briggs: "A prime reason p-values were embraced was that they made automatic, universal decisions about whether to ‚Äúdrop‚Äù variables or to keep them (in a given model schema). But probability is not decision; p-values con ated the concepts. ...  Since decision is relative there is thus no universal solution to variable selection"
- Ioannidis: most scientists (and in particular psychologists, biomedical scientists, social scientists, cognitive scientists and neuroscientists) are still near exclusively educated in NHST; NHST should be abandoned as the de factor cornerstone of research; It is a frequent misconception that a lower p value always means stronger evidence irrespective of the sample size and effect size;  The problem is that usually none of these alternative approaches are taught properly in statistics courses for students in psychology, neuroscience, biomedical science and social science.
-"by construction, 10 inpendendent variables were ..."
-Yarkoni/Westfall: research is typically evaluated based either on ‚Äúgoodness of fit‚Äù between the statistical model and the sample data, or on whether the sizes and directions of certain regression coefficients match what is implied by different theoretical perspectives. As we elaborate below, such demonstrations provide no guarantee of predictive accuracy for out-of- sample data; indeed, in some cases, the pursuit of theoretical parsimony can actually reduce the likelihood of generating good prediction; there is mounting evidence from the ongoing replication crisis that the published results of many papers in psychology do not, in fact, hold up when the same experiments and analyses are independently conducted at a later date; The problem lies in the inference that the parameter estimates obtained in the sample at hand‚Äîi.e., the values ùëè( = 1.6, ùëè# = 0.35, and ùëè$ = 0.62‚Äîwill perform comparably well when applied to other samples drawn from the same population.;  every pattern that could be observed in a given dataset reflects some (generally unknown) combination of signal and error
-" medical research is so extensive, and the stakes are so high"
-Donoho2015: In the future, scientific methodology will be validated empirically. Code sharing and data sharing will allow large numbers of datasets and analysis workflows to be derived from studies science-wide. These will be curated into corpora of data and of workflows. Performance of statistical and machine learning methods will thus ultimately rely on the cross-study and cross-workflow approaches 
-‚ÄúData science has become a fourth approach to scientific discovery, in addition to experimentation, modeling, and computation,‚Äù said Provost Martha Pollack.

- Lo2015: Thus far, genome-wide association studies (GWAS) have been dis- appointing in the inability of investigators to use the results of identified, statistically significant variants in complex diseases to make predictions useful for personalized medicine; We point out that this problem is prevalent in simple as well as com- plex data; higher sig- nificance cannot automatically imply stronger predictivity and il- lustrate through simulations ; "what makes vari- ables good for prediction versus significance depends on different properties of the underlying distributions"; "classical significance test-based approaches"; Real examples are more difficult because the researcher must rely on a limited number of individuals to infer the relevant distributions and the number of possible variables is huge; In fact, significance was not originally designed for the purposes of prediction; a key difference between what makes a variable highly significant versus highly predictive lies in dif- ferent properties of their underlying distributions; prediction has served more in identifying future data behavior
-Cohen1990: " I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data"
  *  hypothesis testing has been greatly overemphasized in psychology and in the other disciplines that use it.
  * In his brilliant analysis of what he called the "infer- ence revolution" in psychology, Gerd Gigerenzer showed how and why no single royal road of drawing conclusions from data is possible
  * Skinner (1957) thought of dedicating his book Verbal Be- havior (and I quote) "to the statisticians and scientific methodologists with whose help this book would never have been completed" (p. 111)





CAVEATS / COUNTER-ARGUMENTS:
- One may wonder whether the shortcoming of using signifi- cance is due to the custom of using marginal significance and not taking into account the possible interaction effects of groups of variables.
- important question is also significance-based filtering for subsequent prediction







DISCUSSION:

- vocabulary/expressions: we are in the usual linear regression setup; the selection event; 

- biggest abberations between prediction and inference when a) the assumed model did not correspond to the ground-throuth process that generated the data and b) correlation between the input variables -> however, this was likely to be the most realistic simulation in medicine, neuroscience, and genetics where we have complex biology-phenotype interactions that are unlikely to correspond to the linear GLM models that we use in everyday work

- potentially related to the discussion of whether pvales for prediction performances make sense: cf. Evernote on pvalues in DNNs

- The lasso with k nonzero coefficients has k degrees of freedom in expectation




Discussion with Olivier March 8:

- our goal is different from post-selection inference: In post-sel inf we have already gone through a filter process, whether it be least-angle regression or PCA or Lasso, after which we want to know among the chosen subset of variables which are *also* significant

- "modelling error" (vs. "measurement error"): The most common case in biomedicine is probably that the assumed linear model is wrong and does not correspond to the real data mechanisms of how the observations arose. In genetics for example, using logistic-regression-like GWAS analyses we can often only explain very small fractions of the variance related to the disease phenotype in psychiatry and other complex dieases:

We looked for both additive effects which can be seen as the combined effect produced by different symptoms on schizophrenia severity is equal to the sum of their separate effect and interaction effects which mean that the combined effect is not additive. In fact, it is widely assumed that higher-order interactions between vulnerabilities triggered by the environment such as growing up in an urbanized area (Van Os and Kapur, 2009) and vulnerabilities conferred by genes such as NRG1 (Harrison and Weinberger, 2005) are important in the etiology of schizophrenia and may result in this major psychiatric disorder (Van Os and Kapur, 2009). Nonetheless, the very successful genome-wide association studies (GWAS) have been mostly grounded in additive models and thus blind to such interaction effects. In other words, common GWAS applications investigate the separate effect of each individual gene on overall disease vulnerability.




Bertrand March 9:
- random forests -> feature importance
- applications: cross-correlations -> significance is divided between the variables -> more difficult for each individual variable to be significant
- shrinkage ~ few variables
- forward step-wise selection for selection based on p-value (instead of prediction): faisable
- SNR: L2(X_beta) / L2(epsilon) -> ratio more informative
- Lasso: does shrinkage AND selction -> difficult to separate -> once the non-zero betas are set, one could refit an OLS bsed on the subset of identified betas for evaluation out of sample because this would allow to separate/disembiguate/disentangle shrinkage and selection in the evaluation of an out-of-sample performance
- 100 samples, 40 variables, error = none -> we should still have 5% false positives
- aberration by pertubration is weak vs strong condiction on: monotony preserved or not ?
- post-selection inference does not really work and the solutions from Stanford should not be really trusted
- perturbation: I would apply that to Y rather than specific input variables [In sklearn example: perturbation is however also applied to X directly]
- your simulations are long-data scenarios that are not settings where we would naturally choose a Lasso -> not 100% naturalistic setting
- a fundamemntal issue is that significance and variable selection are *binary* metrics, while the process and the underlying computation is actually continuous in nature
-> OLS/inference has literally no researchers degrees of freedom <=> but there are naturally so many more researchers degrees of freedom when computing Lasso, CV, and choosing the lambda
-> the simulations are less convincing than the applications to the real datasets: you should understand the real data better (SNR can be computed even if the ground truth model is not known + quantify the correlation between the inputs by cross-correlation plots)
-> compare to random forests feature importances because these do not have shrinkage effects





Daniel Margulies March 11:
- significant variables are not automatically predictive? Why not? That is wierd?



Discussion with Olivier March 12:
- "forward"-variant of RFE with RF
- Partial dependence plots on variables suspected to have non-linear 
- true vs predicted / plot avec OLS and optimal Lasso and RF
- RF better than Lasso -> indicators of non-linearity
- MAD is more interpretable
- polynomial expansion -> feature selection f_regression -> test
- parameter search with gradient boosting











Discussion with Gael:
- first selected variable in fwd stepwise and Lasso is expectedly the same, because there they are selected marginally; since there is not other potetnial in the model yet -> then bias of L1 will increase in the sense of bias/variance tradeoff
- prediction: the point is this: Do I bring in new information to solve my prediction problem or not? So this is related to the ground truth correlation structure of the input features
- let's write the paper in GDocs please, this is the most practical for me




Discussion with Denis:
- invent and popularize the "inference-prediction plot"
- 




Discussion with Demian:
- The Achilles heel is: any variable significant variable will also be predictive
-> we need to show case where not predictive but strongly significant ("hoechstsignifikant")
- The whole inf-versus-pred problem goes away when we deal with a very strong effect
-> but in these scenario we probably do not need statistics to observe the effect
- we should consider making the -log(p) axis rigid to increase comparability




Discussion with Guillaume March 25:
- Lasso: Why do we not delete one variable at each step? -> shirnkage + selection √† la fois
- estimation is common -> then inference or prediction in different paths of analysis
- p-values (implicitly also effectr size) are in-sample <=> prediction is out-of-sample sample
- !!! excellent final phrase for the conclusion: Claude Bernard (father of experimental research in medicine) / trait√© de la medicine experimentale; ~1865: "medicine professionelle" (no theory, practically oriented) <=> "medcin scientific" (principle, determinism absolu) => Is considered to be the founder of experimental/scientific medicine
- CAVE Why are certain variables with a high absolute coefficient but kicked out before other variables with high absolute coefficients
- OLS and Lasso both express the relation between inputs and outputs as Y = X1 * beta1 + X2 * beta2 -> so the model family is the same, however the goal statistcal investigation is different when using this same mathematical model





Applications:
Mini-mental / CamCAN
/ HCP
/ UKBB



Target journals:
- Nature Methods
- PLoS Medicine (IF 13.5)
- PNAS



JBP? Nichols?
