TODO:
- Jerome: try a non-Gaussian (=pathological) error term
- Lasso/plot: show vertical of automatically selected model
- classification: balanced accuracy
- do ypredicted versus ytrue 45 degree plots on particular examples
- compute+plot also out-of-sample prediction performance for the linear regression



1. FEV - prediction ignores the significant ones
2. birth weight -
3. Prostate - not significant but predictive
-> leave simulations + classification for the exectuable notebooks only?



Titles:
On the inference-prediction dilemma in biomedicine


Main conclusions
1) across simulations and applications, inference/GLM tends to yield more false positives, while prediction yielded more false negatives (coherent with the parsimony constraint of the Lasso)
2) all four possible cases occur in practice:
  * significant and predictive
  * significant but not predictive
  * not significant but predictive
  * not significant and not predictive
3) Efron & Tibshirani 1993: Statististcians work at the intersection of empirical research,
science, and philosophy -> this project tried to emphasize the importance of the philsophical
component of medical research activity. Just because the methods that we use automatically
generate results does not preclude the necessity of questioning the obtained results and
conclusions; just because a tool gives answsers, does not mean that the tool has been an
appropriate choice for the underlying research question

-> "Modeling for prediction and modeling for inference ask research questions in importantly different ways."



Interesting points from the literature:
- Breiman: " “Importance” does not yet have a satisfactory the- oretical definition (I haven’t been able to locate the article Brad references but I’ll keep looking). It depends on the dependencies between the output variable and the input variables, and on the depen- dencies between the input variables. The problem begs for research.
"
- „What meaning can one give to statements that “variable X is important or not impor- tant.” This has puzzled me on and off for quite a while… variable importance has always been defined operationally.  My definition of variable importance is based on prediction. A variable might be considered important if deleting it seriously affects prediction accuracy.
-From clinical trials to epidemiology, p-values have long been used to back claims for the discovery of real e ects amid noisy data.
-Many journalists spotted the obvious connection between the unreliability of p-values and one of the biggest scienti c controversies of our time: the replication crisis,
-as Wasserstein has put it, to “steer research into a post p < 0.05 era”. 
-prediction is causin of ifnerence, not a brother or sister -> inference is a differently nuanced alternative
-keyword: reproducibility
--ASA:  Statistical significance is not equivalent to scientific, human, or economic significance. Smaller p-values do not necessarily imply the presence of larger or more important effects, and larger p- values do not imply a lack of importance or even lack of effect. Any effect, no matter how tiny, can produce a small p-value if the sample size or measurement precision is high enough, and large effects may produce unimpressive p-values if the sample size is small or measurements are imprecise. Similarly, identical estimated effects will have different p-values if the precision of the estimates differs.
-ASA: "No single index should substitute for scientific reasoning."
-Gelman: "really, any p-value) can be viewed as a crude measure of sample size, and this can be framed as the distinction between"
-ASA/Briggs: "A prime reason p-values were embraced was that they made automatic, universal decisions about whether to “drop” variables or to keep them (in a given model schema). But probability is not decision; p-values con ated the concepts. ...  Since decision is relative there is thus no universal solution to variable selection"
- Ioannidis: most scientists (and in particular psychologists, biomedical scientists, social scientists, cognitive scientists and neuroscientists) are still near exclusively educated in NHST; NHST should be abandoned as the de factor cornerstone of research; It is a frequent misconception that a lower p value always means stronger evidence irrespective of the sample size and effect size;  The problem is that usually none of these alternative approaches are taught properly in statistics courses for students in psychology, neuroscience, biomedical science and social science.




Discussion:

- vocabulary/expressions: we are in the usual linear regression setup; the selection event; 

- biggest abberations between prediction and inference when a) the assumed model did not correspond to the ground-throuth process that generated the data and b) correlation between the input variables -> however, this was likely to be the most realistic simulation in medicine, neuroscience, and genetics where we have complex biology-phenotype interactions that are unlikely to correspond to the linear GLM models that we use in everyday work

- potentially related to the discussion of whether pvales for prediction performances make sense: cf. Evernote on pvalues in DNNs

- The lasso with k nonzero coefficients has k degrees of freedom in expectation




Discussion with Olivier March 8:

- our goal is different from post-selection inference: In post-sel inf we have already gone through a filter process, whether it be least-angle regression or PCA or Lasso, after which we want to know among the chosen subset of variables which are *also* significant

- "modelling error" (vs. "measurement error"): The most common case in biomedicine is probably that the assumed linear model is wrong and does not correspond to the real data mechanisms of how the observations arose. In genetics for example, using logistic-regression-like GWAS analyses we can often only explain very small fractions of the variance related to the disease phenotype in psychiatry and other complex dieases:

We looked for both additive effects which can be seen as the combined effect produced by different symptoms on schizophrenia severity is equal to the sum of their separate effect and interaction effects which mean that the combined effect is not additive. In fact, it is widely assumed that higher-order interactions between vulnerabilities triggered by the environment such as growing up in an urbanized area (Van Os and Kapur, 2009) and vulnerabilities conferred by genes such as NRG1 (Harrison and Weinberger, 2005) are important in the etiology of schizophrenia and may result in this major psychiatric disorder (Van Os and Kapur, 2009). Nonetheless, the very successful genome-wide association studies (GWAS) have been mostly grounded in additive models and thus blind to such interaction effects. In other words, common GWAS applications investigate the separate effect of each individual gene on overall disease vulnerability.




Bertrand March 9:
- random forests -> feature importance
- applications: cross-correlations -> significance is divided between the variables -> more difficult for each individual variable to be significant
- shrinkage ~ few variables
- forward step-wise selection for selection based on p-value (instead of prediction): faisable
- SNR: L2(X_beta) / L2(epsilon) -> ratio more informative
- Lasso: does shrinkage AND selction -> difficult to separate -> once the non-zero betas are set, one could refit an OLS bsed on the subset of identified betas for evaluation out of sample because this would allow to separate/disembiguate/disentangle shrinkage and selection in the evaluation of an out-of-sample performance
- 100 samples, 40 variables, error = none -> we should still have 5% false positives
- aberration by pertubration is weak vs strong condiction on: monotony preserved or not ?
- post-selection inference does not really work and the solutions from Stanford should not be really trusted
- perturbation: I would apply that to Y rather than specific input variables [In sklearn example: perturbation is however also applied to X directly]
- your simulations are long-data scenarios that are not settings where we would naturally choose a Lasso -> not 100% naturalistic setting
- a fundamemntal issue is that significance and variable selection are *binary* metrics, while the process and the underlying computation is actually continuous in nature
-> OLS/inference has literally no researchers degrees of freedom <=> but there are naturally so many more researchers degrees of freedom when computing Lasso, CV, and choosing the lambda
-> the simulations are less convincing than the applications to the real datasets: you should understand the real data better (SNR can be computed even if the ground truth model is not known + quantify the correlation between the inputs by cross-correlation plots)
-> compare to random forests feature importances because these do not have shrinkage effects





Daniel Margulies March 11:
- significant variables are not automatically predictive? Why not? That is wierd?



Discussion with Olivier March 12:
- "forward"-variant of RFE with RF
- Partial dependence plots on variables suspected to have non-linear 
- true vs predicted / plot avec OLS and optimal Lasso and RF
- RF better than Lasso -> indicators of non-linearity
- MAD is more interpretable
- polynomial expansion -> feature selection f_regression -> test
- parameter search with gradient boosting











Discussion with Gael:
- first selected variable in fwd stepwise and Lasso is expectedly the same, because there they are selected marginally; since there is not other potetnial in the model yet -> then bias of L1 will increase in the sense of bias/variance tradeoff
- prediction: the point is this: Do I bring in new information to solve my prediction problem or not? So this is related to the ground truth correlation structure of the input features
- let's write the paper in GDocs please, this is the most practical for me




Discussion with Denis:
- invent and popularize the "inference-prediction plot"
- 



Discussion with Demian:
- The Achilles heel is: any variable significant variable will also be predictive
-> we need to show case where not predictive but strongly significant ("hoechstsignifikant")
- The whole inf-versus-pred problem goes away when we deal with a very strong effect
-> but in these scenario we probably do not need statistics to observe the effect
- we should consider making the -log(p) axis rigid to increase comparability




Applications:
Mini-mental / CamCAN
/ HCP
/ UKBB



Target journals:
- Nature Methods
- PLoS Medicine (IF 13.5)
- PNAS



JBP? Nichols?
